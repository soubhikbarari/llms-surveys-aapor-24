# How to Use NLP and Generative AI in Survey Research: Foundations Quiz

## Questions

###  1. When would zero-shot learning be preferable to few-shot learning? (Select all that apply)

- A. When there are no labeled examples available for a task.
- B. When the task is highly specific and requires domain expertise.
- C. When you want to minimize human effort in training or fine-tuning.
- D. When the task requires training the model quickly with only a few examples.
- E. When you want to achieve the best possible accuracy on a complex task.

### 2. How could Gen AI be used to reduce measurement error in surveys? (Select all that apply)

- A. By processing the wording of questions to remove/alter ambiguous phrases.
- B. By randomly auto-generating responses for respondents.
- C. By using adaptive probing for more accurate responses.
- D. By randomly altering the complexity of a question text for respondents.
- E. By detecting and flagging inconsistent or illogical responses in real time.

### 3. What is the role of attention in large language models?

- A. To focus the model on the most relevant parts of input text when making predictions.
- B. To allow the model to learn associations across distant words in a sentence.
- C. To enhance the model's speed by ignoring less important text in a sentence.
- D. To increase the model's ability to generalize by training it on multiple tasks simultaneously.
- E. To ensure the model can capture local relationships between neighboring words.

### 4. You ran a topic model on all questions asked by the Pew Research Center in 2024. How would you validate the topics generated by this model?

- A. By checking for topic coherence using relevant metrics.
- B. By asking subject matter experts to review and label the topics.
- C. By conducting a survey of respondents to validate the topic relevance.
- D. By comparing the generated topics to a benchmark dataset of known topics.
- E. By rerunning the model multiple times to ensure consistency in the results.

## Answers

### 1. When would zero-shot learning be preferable to few-shot learning?

#### Correct answers:
- A. When there are no labeled examples available for a task.
- C. When you want to minimize human effort in training or fine-tuning.

#### Explanation:
- A: Zero-shot learning is designed for tasks where no labeled examples are available, making it preferable when labeled data doesn't exist.
- C: Zero-shot learning minimizes human effort by not requiring any task-specific examples for training, making it useful when labeled data collection isn't feasible.
- B, D, E are incorrect because zero-shot learning struggles with highly specific tasks or achieving the best possible accuracy without any labeled data. Few-shot learning would be preferable in those cases.

### 2. How could Gen AI be used to reduce measurement error in surveys?

#### Correct answers:

- A. By processing the wording of questions to remove/alter ambiguous phrases.
- C. By using adaptive probing for more accurate responses.
- E. By detecting and flagging inconsistent or illogical responses in real time.

### Explanation:

- A: Generative AI can recommend clearer question wording, reducing ambiguity and potential misunderstanding.
- C: Adaptive probing from LLMs can clarify responses and help capture more accurate data.
- E: AI can detect and flag inconsistent responses in real time, helping reduce errors during data collection.
- B and D are incorrect because generating responses or randomly changing question text would increase bias and measurement error, as they compromise the authenticity and construct validity of the collected data.

### 3. What is the role of attention in large language models?

#### Correct answers:

- A. To focus the model on the most relevant parts of input text when making predictions.
- B. To allow the model to learn associations across distant words in a sentence.

#### Explanation:

- A: Attention mechanisms allow the model to weigh the importance of different parts of the input, helping it focus on the most relevant text for better predictions.
- B: Attention helps models capture long-range dependencies, allowing them to understand relationships between distant words.
- C, D, and E are incorrect because attention doesn't ignore text, train across multiple tasks, or exclusively focus on local relationships (itâ€™s meant for both local and distant dependencies).

### 4. You ran a topic model on all questions asked by the Pew Research Center in 2024. How would you validate the topics generated by this model?

#### Correct answers:

- A. By checking for topic coherence using relevant metrics like PMI or NPMI.
- B. By asking subject matter experts to review and label the topics.
- E. By rerunning the model multiple times to ensure consistency in the results.

#### Explanation:

- A: Topic coherence metrics like PMI or NPMI are standard ways to check how well the words in a topic cohere semantically.
- B: Subject matter experts can review and label topics, ensuring they make sense in the context of the survey.
- E: Rerunning the model ensures the topics generated are consistent and not dependent on random initialization or data noise.
- C and D are incorrect because conducting a survey is not a common way to validate topics, and there isn't usually a benchmark dataset of known topics for comparison in unsupervised learning.


